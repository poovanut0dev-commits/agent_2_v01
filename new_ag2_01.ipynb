{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa59373-6f23-4338-b2ca-aaf8752bd74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_URL = \"https://api.ollama.services.storemesh.com/\"\n",
    "MODEL_NAME = \"gpt-oss:20b\"\n",
    "\n",
    "LLM_URL = \"http://localhost:11434\"\n",
    "MODEL_NAME = \"gpt-oss:20b\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309442d3-4066-4a4c-8984-1a8d30b293e2",
   "metadata": {},
   "source": [
    "# ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö ‚Äú‡πÅ‡∏¢‡∏Å‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09fe3371-7969-4fea-afac-58cfee13df50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "\n",
    "# ‡∏ï‡∏±‡∏ß‡∏ó‡∏î‡∏•‡∏≠‡∏á‡∏ó‡∏µ‡πà 1: ‡πÅ‡∏¢‡∏Å Page ‚Üí Paragraph (‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î)\n",
    "def extract_paragraphs(pdf_path: str):\n",
    "    reader = PdfReader(pdf_path)\n",
    "    results = []\n",
    "\n",
    "    for page_idx, page in enumerate(reader.pages):\n",
    "        text = page.extract_text()\n",
    "        if not text:\n",
    "            continue\n",
    "\n",
    "        paragraphs = [\n",
    "            p.strip()\n",
    "            for p in text.split(\"\\n\")\n",
    "            if len(p.strip()) > 5\n",
    "        ]\n",
    "\n",
    "        for p in paragraphs:\n",
    "            results.append({\n",
    "                \"doc_id\": pdf_path,\n",
    "                \"page\": page_idx + 1,\n",
    "                \"block_type\": \"paragraph\",\n",
    "                \"section\": None,\n",
    "                \"text\": p\n",
    "            })\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3ef8fe6-99f9-49e5-9913-e4500bb15ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pypdf import PdfReader\n",
    "\n",
    "def extract_with_sections(pdf_path: str):\n",
    "    reader = PdfReader(pdf_path)\n",
    "    results = []\n",
    "    current_section = \"Unknown\"\n",
    "\n",
    "    for page_idx, page in enumerate(reader.pages):\n",
    "        text = page.extract_text()\n",
    "        if not text:\n",
    "            continue\n",
    "\n",
    "        lines = text.split(\"\\n\")\n",
    "\n",
    "        for line in lines:\n",
    "            clean = line.strip()\n",
    "\n",
    "            # heuristic: heading ‡∏ï‡∏±‡∏ß‡πÉ‡∏´‡∏ç‡πà / ‡∏°‡∏µ‡πÄ‡∏•‡∏Ç‡∏ô‡∏≥\n",
    "            if re.match(r\"^[0-9]+\\.\", clean) or clean.isupper():\n",
    "                current_section = clean\n",
    "                results.append({\n",
    "                    \"doc_id\": pdf_path,\n",
    "                    \"page\": page_idx + 1,\n",
    "                    \"block_type\": \"heading\",\n",
    "                    \"section\": current_section,\n",
    "                    \"text\": clean\n",
    "                })\n",
    "                continue\n",
    "\n",
    "            if len(clean) > 5:\n",
    "                results.append({\n",
    "                    \"doc_id\": pdf_path,\n",
    "                    \"page\": page_idx + 1,\n",
    "                    \"block_type\": \"paragraph\",\n",
    "                    \"section\": current_section,\n",
    "                    \"text\": clean\n",
    "                })\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f329ded8-54f4-4938-b473-d2dacd12adf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "\n",
    "def extract_tables(pdf_path: str):\n",
    "    results = []\n",
    "\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page_idx, page in enumerate(pdf.pages):\n",
    "            tables = page.extract_tables() or []\n",
    "\n",
    "            for t_idx, table in enumerate(tables):\n",
    "                flat_rows = []\n",
    "\n",
    "                for row in table:\n",
    "                    if not row:\n",
    "                        continue\n",
    "\n",
    "                    # üîë FIX: ‡∏Å‡∏£‡∏≠‡∏á None + cast ‡πÄ‡∏õ‡πá‡∏ô str\n",
    "                    clean_cells = [\n",
    "                        str(cell).strip()\n",
    "                        for cell in row\n",
    "                        if cell is not None and str(cell).strip() != \"\"\n",
    "                    ]\n",
    "\n",
    "                    if clean_cells:\n",
    "                        flat_rows.append(\" | \".join(clean_cells))\n",
    "\n",
    "                if flat_rows:\n",
    "                    results.append({\n",
    "                        \"doc_id\": pdf_path,\n",
    "                        \"page\": page_idx + 1,\n",
    "                        \"block_type\": \"table\",\n",
    "                        \"section\": None,\n",
    "                        \"text\": \" ; \".join(flat_rows)\n",
    "                    })\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47518d44-c3f8-4034-b4d7-04cd4ce90f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        doc_id  page block_type  section                                  text\n",
      "0  dsi2566.pdf     1  paragraph  Unknown              ‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏®‡∏≤‡∏™‡∏ï‡∏£‡∏ö‡∏±‡∏ì‡∏ë‡∏¥‡∏ï\n",
      "1  dsi2566.pdf     1  paragraph  Unknown  ‡∏™‡∏≤‡∏Ç‡∏≤‡∏ß‡∏¥‡∏ä‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡πÅ‡∏•‡∏∞‡∏ô‡∏ß‡∏±‡∏ï‡∏Å‡∏£‡∏£‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n",
      "2  dsi2566.pdf     1  paragraph  Unknown          (‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á ‡∏û.‡∏®. 2566)\n",
      "3  dsi2566.pdf     1  paragraph  Unknown                   ‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡∏û‡∏´‡∏∏‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏Å‡∏≤‡∏£\n",
      "4  dsi2566.pdf     1  paragraph  Unknown            ‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡πà‡∏ß‡∏°‡∏°‡∏∑‡∏≠‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pdf_path = \"dsi2566.pdf\"\n",
    "\n",
    "paragraphs = extract_with_sections(pdf_path)\n",
    "tables = extract_tables(pdf_path)\n",
    "\n",
    "structured = paragraphs + tables\n",
    "df_str = pd.DataFrame(structured)\n",
    "\n",
    "print(df_str.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0227180d-288c-4856-95fe-bd000d275dc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>page</th>\n",
       "      <th>block_type</th>\n",
       "      <th>section</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dsi2566.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>paragraph</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏®‡∏≤‡∏™‡∏ï‡∏£‡∏ö‡∏±‡∏ì‡∏ë‡∏¥‡∏ï</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dsi2566.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>paragraph</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>‡∏™‡∏≤‡∏Ç‡∏≤‡∏ß‡∏¥‡∏ä‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡πÅ‡∏•‡∏∞‡∏ô‡∏ß‡∏±‡∏ï‡∏Å‡∏£‡∏£‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dsi2566.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>paragraph</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>(‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á ‡∏û.‡∏®. 2566)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dsi2566.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>paragraph</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡∏û‡∏´‡∏∏‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏Å‡∏≤‡∏£</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dsi2566.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>paragraph</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡πà‡∏ß‡∏°‡∏°‡∏∑‡∏≠‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2748</th>\n",
       "      <td>dsi2566.pdf</td>\n",
       "      <td>73</td>\n",
       "      <td>table</td>\n",
       "      <td>None</td>\n",
       "      <td>‡∏´‡∏°‡∏ß‡∏î‡∏ß‡∏¥‡∏ä‡∏≤ | ‡∏£‡∏´‡∏±‡∏™‡∏ß‡∏¥‡∏ä‡∏≤ | ‡∏£‡∏≤‡∏¢‡∏ß‡∏¥‡∏ä‡∏≤ | ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2749</th>\n",
       "      <td>dsi2566.pdf</td>\n",
       "      <td>74</td>\n",
       "      <td>table</td>\n",
       "      <td>None</td>\n",
       "      <td>‡∏´‡∏°‡∏ß‡∏î‡∏ß‡∏¥‡∏ä‡∏≤ | ‡∏£‡∏´‡∏±‡∏™‡∏ß‡∏¥‡∏ä‡∏≤ | ‡∏£‡∏≤‡∏¢‡∏ß‡∏¥‡∏ä‡∏≤ | ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2750</th>\n",
       "      <td>dsi2566.pdf</td>\n",
       "      <td>75</td>\n",
       "      <td>table</td>\n",
       "      <td>None</td>\n",
       "      <td>‡∏´‡∏°‡∏ß‡∏î‡∏ß‡∏¥‡∏ä‡∏≤ | ‡∏£‡∏´‡∏±‡∏™‡∏ß‡∏¥‡∏ä‡∏≤ | ‡∏£‡∏≤‡∏¢‡∏ß‡∏¥‡∏ä‡∏≤ | ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2751</th>\n",
       "      <td>dsi2566.pdf</td>\n",
       "      <td>76</td>\n",
       "      <td>table</td>\n",
       "      <td>None</td>\n",
       "      <td>‡∏´‡∏°‡∏ß‡∏î‡∏ß‡∏¥‡∏ä‡∏≤ | ‡∏£‡∏´‡∏±‡∏™‡∏ß‡∏¥‡∏ä‡∏≤ | ‡∏£‡∏≤‡∏¢‡∏ß‡∏¥‡∏ä‡∏≤ | ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2752</th>\n",
       "      <td>dsi2566.pdf</td>\n",
       "      <td>77</td>\n",
       "      <td>table</td>\n",
       "      <td>None</td>\n",
       "      <td>‡∏´‡∏°‡∏ß‡∏î‡∏ß‡∏¥‡∏ä‡∏≤\\n‡πÅ‡∏•‡∏∞‡∏™‡∏´‡∏Å‡∏¥‡∏à‡∏®‡∏∂‡∏Å‡∏©‡∏≤ | ‡∏£‡∏´‡∏±‡∏™‡∏ß‡∏¥‡∏ä‡∏≤ | ‡∏£‡∏≤‡∏¢‡∏ß‡∏¥‡∏ä‡∏≤ |...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2753 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           doc_id  page block_type  section  \\\n",
       "0     dsi2566.pdf     1  paragraph  Unknown   \n",
       "1     dsi2566.pdf     1  paragraph  Unknown   \n",
       "2     dsi2566.pdf     1  paragraph  Unknown   \n",
       "3     dsi2566.pdf     1  paragraph  Unknown   \n",
       "4     dsi2566.pdf     1  paragraph  Unknown   \n",
       "...           ...   ...        ...      ...   \n",
       "2748  dsi2566.pdf    73      table     None   \n",
       "2749  dsi2566.pdf    74      table     None   \n",
       "2750  dsi2566.pdf    75      table     None   \n",
       "2751  dsi2566.pdf    76      table     None   \n",
       "2752  dsi2566.pdf    77      table     None   \n",
       "\n",
       "                                                   text  \n",
       "0                              ‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏®‡∏≤‡∏™‡∏ï‡∏£‡∏ö‡∏±‡∏ì‡∏ë‡∏¥‡∏ï  \n",
       "1                  ‡∏™‡∏≤‡∏Ç‡∏≤‡∏ß‡∏¥‡∏ä‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡πÅ‡∏•‡∏∞‡∏ô‡∏ß‡∏±‡∏ï‡∏Å‡∏£‡∏£‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•  \n",
       "2                          (‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á ‡∏û.‡∏®. 2566)  \n",
       "3                                   ‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡∏û‡∏´‡∏∏‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏Å‡∏≤‡∏£  \n",
       "4                            ‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡πà‡∏ß‡∏°‡∏°‡∏∑‡∏≠‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á  \n",
       "...                                                 ...  \n",
       "2748  ‡∏´‡∏°‡∏ß‡∏î‡∏ß‡∏¥‡∏ä‡∏≤ | ‡∏£‡∏´‡∏±‡∏™‡∏ß‡∏¥‡∏ä‡∏≤ | ‡∏£‡∏≤‡∏¢‡∏ß‡∏¥‡∏ä‡∏≤ | ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢...  \n",
       "2749  ‡∏´‡∏°‡∏ß‡∏î‡∏ß‡∏¥‡∏ä‡∏≤ | ‡∏£‡∏´‡∏±‡∏™‡∏ß‡∏¥‡∏ä‡∏≤ | ‡∏£‡∏≤‡∏¢‡∏ß‡∏¥‡∏ä‡∏≤ | ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢...  \n",
       "2750  ‡∏´‡∏°‡∏ß‡∏î‡∏ß‡∏¥‡∏ä‡∏≤ | ‡∏£‡∏´‡∏±‡∏™‡∏ß‡∏¥‡∏ä‡∏≤ | ‡∏£‡∏≤‡∏¢‡∏ß‡∏¥‡∏ä‡∏≤ | ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢...  \n",
       "2751  ‡∏´‡∏°‡∏ß‡∏î‡∏ß‡∏¥‡∏ä‡∏≤ | ‡∏£‡∏´‡∏±‡∏™‡∏ß‡∏¥‡∏ä‡∏≤ | ‡∏£‡∏≤‡∏¢‡∏ß‡∏¥‡∏ä‡∏≤ | ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢...  \n",
       "2752  ‡∏´‡∏°‡∏ß‡∏î‡∏ß‡∏¥‡∏ä‡∏≤\\n‡πÅ‡∏•‡∏∞‡∏™‡∏´‡∏Å‡∏¥‡∏à‡∏®‡∏∂‡∏Å‡∏©‡∏≤ | ‡∏£‡∏´‡∏±‡∏™‡∏ß‡∏¥‡∏ä‡∏≤ | ‡∏£‡∏≤‡∏¢‡∏ß‡∏¥‡∏ä‡∏≤ |...  \n",
       "\n",
       "[2753 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9f14466c-dd69-4bde-a2f8-13d8c884b8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏õ‡πá‡∏ô CSV\n",
    "df.to_csv(\"dsi2566_structured.csv\", index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5d4635-3924-44b3-a6b3-14683dcacf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Curriculum QA Agent (DSPy ReAct + Ollama)\n",
    "# FINAL / CLEAN / RUNNABLE\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import hashlib\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "import dspy\n",
    "\n",
    "# ============================================================\n",
    "# 1) CONFIG\n",
    "# ============================================================\n",
    "\n",
    "EMBEDDING_API = \"http://localhost:11434/api/embeddings\"\n",
    "EMBED_MODEL = \"bge-m3:567m\"\n",
    "\n",
    "LLM_URL = \"https://api.ollama.services.storemesh.com/\"\n",
    "MODEL_NAME = \"gpt-oss:20b\"\n",
    "\n",
    "TOP_K = 6\n",
    "CACHE_DIR = \"cache\"\n",
    "CACHE_NAME = \"curriculum_embeddings\"\n",
    "\n",
    "# ============================================================\n",
    "# 2) EMBEDDING CACHE\n",
    "# ============================================================\n",
    "\n",
    "def corpus_fingerprint(texts: list[str]) -> str:\n",
    "    h = hashlib.sha256()\n",
    "    for t in texts:\n",
    "        h.update(t.encode(\"utf-8\"))\n",
    "    return h.hexdigest()\n",
    "\n",
    "\n",
    "def embed_texts_with_cache(texts: list[str]) -> np.ndarray:\n",
    "    os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "    emb_path = os.path.join(CACHE_DIR, f\"{CACHE_NAME}.npy\")\n",
    "    hash_path = os.path.join(CACHE_DIR, f\"{CACHE_NAME}.hash\")\n",
    "\n",
    "    current_hash = corpus_fingerprint(texts)\n",
    "\n",
    "    # ---- load cache ----\n",
    "    if os.path.exists(emb_path) and os.path.exists(hash_path):\n",
    "        with open(hash_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            saved_hash = f.read().strip()\n",
    "\n",
    "        emb = np.load(emb_path)\n",
    "        if saved_hash == current_hash and emb.shape[0] == len(texts):\n",
    "            print(\"‚úÖ Loaded embeddings from cache\")\n",
    "            return emb\n",
    "\n",
    "        print(\"‚ôª Cache invalid ‚Üí re-embedding\")\n",
    "\n",
    "    # ---- create embeddings ----\n",
    "    vectors = []\n",
    "    print(\"üß† Creating embeddings...\")\n",
    "    for t in tqdm(texts, desc=\"Embedding\", unit=\"chunk\"):\n",
    "        r = requests.post(\n",
    "            EMBEDDING_API,\n",
    "            json={\n",
    "                \"model\": EMBED_MODEL,\n",
    "                \"prompt\": t\n",
    "            },\n",
    "            timeout=120\n",
    "        )\n",
    "        r.raise_for_status()\n",
    "        vectors.append(r.json()[\"embedding\"])\n",
    "\n",
    "    emb = np.array(vectors)\n",
    "\n",
    "    np.save(emb_path, emb)\n",
    "    with open(hash_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(current_hash)\n",
    "\n",
    "    print(\"üíæ Embedding cache saved\")\n",
    "    return emb\n",
    "\n",
    "# ============================================================\n",
    "# 3) BUILD CORPUS FROM DF\n",
    "# ============================================================\n",
    "\n",
    "def build_curriculum_corpus(df: pd.DataFrame):\n",
    "    texts = []\n",
    "    meta = []\n",
    "\n",
    "    for _, r in tqdm(\n",
    "        df.iterrows(),\n",
    "        total=len(df),\n",
    "        desc=\"Building corpus\",\n",
    "        unit=\"block\"\n",
    "    ):\n",
    "        blob = f\"\"\"\n",
    "[SECTION]\n",
    "{r[\"section\"]}\n",
    "\n",
    "[TYPE]\n",
    "{r[\"block_type\"]}\n",
    "\n",
    "[CONTENT]\n",
    "{r[\"text\"]}\n",
    "\"\"\".strip()\n",
    "\n",
    "        texts.append(blob)\n",
    "        meta.append({\n",
    "            \"section\": r[\"section\"],\n",
    "            \"page\": r[\"page\"],\n",
    "            \"block_type\": r[\"block_type\"]\n",
    "        })\n",
    "\n",
    "    return texts, meta\n",
    "\n",
    "# ============================================================\n",
    "# 4) RETRIEVER\n",
    "# ============================================================\n",
    "\n",
    "class CurriculumRetriever:\n",
    "    def __init__(self, texts, meta, embeddings):\n",
    "        self.texts = texts\n",
    "        self.meta = meta\n",
    "        self.embeddings = embeddings\n",
    "\n",
    "    def search(self, query: str, k: int = TOP_K):\n",
    "        q_emb = embed_texts_with_cache([query])\n",
    "        sims = cosine_similarity(q_emb, self.embeddings)[0]\n",
    "        idx = sims.argsort()[-k:][::-1]\n",
    "\n",
    "        return [\n",
    "            {\n",
    "                \"text\": self.texts[i],\n",
    "                \"meta\": self.meta[i],\n",
    "                \"score\": float(sims[i])\n",
    "            }\n",
    "            for i in idx\n",
    "        ]\n",
    "\n",
    "# ============================================================\n",
    "# 5) DSPy SIGNATURE\n",
    "# ============================================================\n",
    "\n",
    "class CurriculumQASignature(dspy.Signature):\n",
    "    \"\"\"\n",
    "    ‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠ The Librarian ‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô Knowledge Management, Learning & Development (L&D)\n",
    "‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ ‡∏Ñ‡πâ‡∏ô‡∏Ñ‡∏∑‡∏ô ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå ‡πÅ‡∏•‡∏∞‡∏™‡∏£‡∏∏‡∏õ‡∏≠‡∏á‡∏Ñ‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£‡πÅ‡∏•‡∏∞‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏∞‡∏ö‡∏ö\n",
    "    \"\"\"\n",
    "    question = dspy.InputField()\n",
    "    context = dspy.InputField()\n",
    "    answer = dspy.OutputField()\n",
    "\n",
    "# ============================================================\n",
    "# 6) REACT AGENT (FIXED: tools=[])\n",
    "# ============================================================\n",
    "\n",
    "class CurriculumQAAgent(dspy.Module):\n",
    "    def __init__(self, retriever):\n",
    "        super().__init__()\n",
    "        self.retriever = retriever\n",
    "\n",
    "        # üîë FIX ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç\n",
    "        self.react = dspy.ReAct(\n",
    "            CurriculumQASignature,\n",
    "            tools=[]\n",
    "        )\n",
    "\n",
    "    def forward(self, question: str):\n",
    "        docs = self.retriever.search(question)\n",
    "\n",
    "        context = \"\\n\\n\".join(\n",
    "            f\"\"\"\n",
    "[Section: {d['meta']['section']} | Page: {d['meta']['page']} | Type: {d['meta']['block_type']}]\n",
    "{d['text'][:1200]}\n",
    "\"\"\".strip()\n",
    "            for d in docs\n",
    "        )\n",
    "\n",
    "        return self.react(\n",
    "            question=question,\n",
    "            context=context\n",
    "        )\n",
    "\n",
    "# ============================================================\n",
    "# 7) MAIN\n",
    "# ============================================================\n",
    "\n",
    "def main(df: pd.DataFrame):\n",
    "    print(\"üìò Building curriculum corpus...\")\n",
    "    corpus_texts, corpus_meta = build_curriculum_corpus(df)\n",
    "\n",
    "    print(\"üß† Loading / creating embeddings...\")\n",
    "    embeddings = embed_texts_with_cache(corpus_texts)\n",
    "\n",
    "    retriever = CurriculumRetriever(\n",
    "        corpus_texts,\n",
    "        corpus_meta,\n",
    "        embeddings\n",
    "    )\n",
    "\n",
    "    print(\"ü§ñ Initializing LLM...\")\n",
    "    lm = dspy.LM(\n",
    "        f\"ollama/{MODEL_NAME}\",\n",
    "        api_base=LLM_URL,\n",
    "        cache=False\n",
    "    )\n",
    "    dspy.configure(lm=lm)\n",
    "\n",
    "    agent = CurriculumQAAgent(retriever)\n",
    "\n",
    "    print(\"\\n‚úÖ Curriculum QA Agent ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô\")\n",
    "    print(\"‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° (exit ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏≠‡∏≠‡∏Å)\\n\")\n",
    "\n",
    "    while True:\n",
    "        q = input(\"‚ùì ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: \").strip()\n",
    "        if q.lower() in [\"exit\", \"quit\"]:\n",
    "            break\n",
    "\n",
    "        result = agent(question=q)\n",
    "        print(\"\\nüìå ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö:\")\n",
    "        print(result.answer)\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "# ============================================================\n",
    "# 8) ENTRY\n",
    "# ============================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á:\n",
    "        df = pd.read_csv(\"curriculum_blocks.csv\")\n",
    "    \"\"\"\n",
    "\n",
    "    # üî¥ ‡πÉ‡∏™‡πà df ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ (‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô pandas DataFrame)\n",
    "    df = df_str\n",
    "\n",
    "    main(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68dc9cf-b21d-4ec0-99a7-5e456453ce40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Initializing LLM...\n",
      "\n",
      "üß† Extracting curriculum structure...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üìê Building global curriculum context: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2753/2753 [00:00<00:00, 26747.18block/s]\n",
      "üß† Curriculum structure reasoning: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:37<00:00, 37.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìê CURRICULUM STRUCTURE (JSON):\n",
      "{\n",
      "  \"institution\": \"‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢‡∏ò‡∏£‡∏£‡∏°‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå\",\n",
      "  \"faculty\": \"‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢‡∏™‡∏´‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏Å‡∏≤‡∏£\",\n",
      "  \"program\": {\n",
      "    \"code\": \"20182067117526\",\n",
      "    \"name_th\": \"‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏®‡∏≤‡∏™‡∏ï‡∏£‡∏ö‡∏±‡∏ì‡∏ë‡∏¥‡∏ï ‡∏™‡∏≤‡∏Ç‡∏≤‡∏ß‡∏¥‡∏ä‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡πÅ‡∏•‡∏∞‡∏ô‡∏ß‡∏±‡∏ï‡∏Å‡∏£‡∏£‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\",\n",
      "    \"name_en\": \"Bachelor of Science Program in Data Science and Innovation\",\n",
      "    \"degree_full_th\": \"‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏®‡∏≤‡∏™‡∏ï‡∏£‡∏ö‡∏±‡∏ì‡∏ë‡∏¥‡∏ï (‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡πÅ‡∏•‡∏∞‡∏ô‡∏ß‡∏±‡∏ï‡∏Å‡∏£‡∏£‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•)\",\n",
      "    \"degree_short_th\": \"‡∏ß‡∏ó.‡∏ö. (‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡πÅ‡∏•‡∏∞‡∏ô‡∏ß‡∏±‡∏ï‡∏Å‡∏£‡∏£‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•)\",\n",
      "    \"degree_full_en\": \"Bachelor of Science (Data Science and Innovation)\",\n",
      "    \"degree_short_en\": \"B.Sc. (Data Science and Innovation)\",\n",
      "    \"level\": \"4 ‡∏õ‡∏µ\",\n",
      "    \"category\": \"‡∏õ‡∏£‡∏¥‡∏ç‡∏ç‡∏≤‡∏ï‡∏£‡∏µ‡∏ó‡∏≤‡∏á‡∏ß‡∏¥‡∏ä‡∏≤‡∏ä‡∏µ‡∏û‡∏´‡∏£‡∏∑‡∏≠‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£\",\n",
      "    \"languages\": [\"‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢\", \"‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©\"],\n",
      "    \"collaboration\": \"‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢‡∏™‡∏´‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏Å‡∏≤‡∏£ ‡πÅ‡∏•‡∏∞ ‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢‡∏ô‡∏ß‡∏±‡∏ï‡∏Å‡∏£‡∏£‡∏°\",\n",
      "    \"degree_award\": \"‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏™‡∏≤‡∏Ç‡∏≤‡∏ß‡∏¥‡∏ä‡∏≤‡πÄ‡∏î‡∏µ‡∏¢‡∏ß\",\n",
      "    \"status\": [\n",
      "      \"‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á ‡∏û.‡∏®. 2566 (‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏à‡∏≤‡∏Å‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏®‡∏≤‡∏™‡∏ï‡∏£‡∏ö‡∏±‡∏ì‡∏ë‡∏¥‡∏ï ‡∏™‡∏≤‡∏Ç‡∏≤‡∏ß‡∏¥‡∏ä‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡πÅ‡∏•‡∏∞‡∏ô‡∏ß‡∏±‡∏ï‡∏Å‡∏£‡∏£‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡∏û.‡∏®.2561)\",\n",
      "      \"‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÄ‡∏õ‡∏¥‡∏î‡∏™‡∏≠‡∏ô‡πÉ‡∏ô‡∏†‡∏≤‡∏Ñ‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏ó‡∏µ‡πà 1 ‡∏õ‡∏µ‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤ 2566\",\n",
      "      \"‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡πÇ‡∏î‡∏¢‡∏Ñ‡∏ì‡∏∞‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏≤‡∏£‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢‡∏ß‡∏¥‡∏ä‡∏≤‡∏Å‡∏≤‡∏£ 2/2566\",\n",
      "      \"‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏≠‡∏ô‡∏∏‡∏°‡∏±‡∏ï‡∏¥‡∏à‡∏≤‡∏Å‡∏™‡∏†‡∏≤‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢ (‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏)\"\n",
      "    ],\n",
      "    \"career_paths\": [\n",
      "      \"Data Scientist\",\n",
      "      \"Data Analyst\",\n",
      "      \"Data Engineer\",\n",
      "      \"Statistic Analyst\",\n",
      "      \"Business Analyst\",\n",
      "      \"Data Architect\",\n",
      "      \"Actuarial Scientist\",\n",
      "      \"Digital Forensic Scientist\"\n",
      "    ],\n",
      "    \"location\": \"‡∏®‡∏π‡∏ô‡∏¢‡πå‡∏£‡∏±‡∏á‡∏™‡∏¥‡∏ï\",\n",
      "    \"cost\": {\n",
      "      \"project_type\": \"‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏û‡∏¥‡πÄ‡∏®‡∏©\",\n",
      "      \"student_fee\": {\n",
      "        \"Thai\": 388215,\n",
      "        \"International\": 388215\n",
      "      }\n",
      "    },\n",
      "    \"admission\": {\n",
      "      \"target\": \"‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÅ‡∏•‡∏∞‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏ï‡πà‡∏≤‡∏á‡∏ä‡∏≤‡∏ï‡∏¥‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÑ‡∏î‡πâ‡∏î‡∏µ\",\n",
      "      \"requirements\": \"‡∏ï‡∏≤‡∏°‡∏Ç‡πâ‡∏≠‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢‡∏ò‡∏£‡∏£‡∏°‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå ‡∏û.‡∏®.2561 ‡∏Ç‡πâ‡∏≠ 14 ‡πÅ‡∏•‡∏∞‡∏£‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ö‡∏Ñ‡∏±‡∏î‡πÄ‡∏•‡∏∑‡∏≠‡∏Å\"\n",
      "    },\n",
      "    \"enrollment_plan\": {\n",
      "      \"annual_quota\": 50,\n",
      "      \"years\": [2566, 2567, 2568, 2569, 2570],\n",
      "      \"students_per_year\": [50, 50, 50, 50, 50],\n",
      "      \"expected_graduation\": [null, null, null, 50, 50]\n",
      "    },\n",
      "    \"philosophy\": \"‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà, AI, Thailand 4.0\"\n",
      "  }\n",
      "}\n",
      "\n",
      "üìò Building retrieval index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üìò Building retrieval corpus: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2753/2753 [00:00<00:00, 18739.43block/s]\n",
      "üß† Creating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2753/2753 [15:14<00:00,  3.01chunk/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Structure-aware Curriculum QA Agent ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô\n",
      "‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£ (exit ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏≠‡∏≠‡∏Å)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "‚ùì ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°:  ‡∏õ‡∏µ 1 ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏≠‡∏∞‡πÑ‡∏£\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üß† Creating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:04<00:00,  4.10s/chunk]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìå ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö:\n",
      "**‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠ / ‡∏õ‡∏£‡∏∞‡πÄ‡∏î‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å**  \n",
      "- ‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡πÉ‡∏ô‡∏õ‡∏µ‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏ó‡∏µ‡πà 1 ‡∏Ç‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡∏∞‡∏ô‡∏ß‡∏±‡∏ï‡∏Å‡∏£‡∏£‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•  \n",
      "- ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏ó‡∏µ‡πà‡∏Ñ‡∏≤‡∏î‡∏´‡∏ß‡∏±‡∏á‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏à‡∏ö‡∏õ‡∏µ‡πÅ‡∏£‡∏Å  \n",
      "\n",
      "**‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£**  \n",
      "\n",
      "| ‡∏†‡∏≤‡∏Ñ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô | ‡∏ß‡∏¥‡∏ä‡∏≤ (‡∏£‡∏´‡∏±‡∏™ ‚Äì ‡∏ä‡∏∑‡πà‡∏≠) | ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï |\n",
      "|---|---|---|\n",
      "| **‡∏†‡∏≤‡∏Ñ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ó‡∏µ‡πà 1** | ‡∏°‡∏ò.100 ‡∏û‡∏•‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏•‡∏á‡∏°‡∏∑‡∏≠‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ | 3 |\n",
      "|  | ‡∏°‡∏ò.106 ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏¥‡∏î‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏£‡∏£‡∏Ñ‡πå‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏™‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏£ | 3 |\n",
      "|  | ‡∏°‡∏ò.107 ‡∏ó‡∏±‡∏Å‡∏©‡∏∞‡∏î‡∏¥‡∏à‡∏¥‡∏ó‡∏±‡∏•‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ | 3 |\n",
      "|  | ‡∏°‡∏ò.108 ‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏ï‡∏ô‡πÄ‡∏≠‡∏á | 3 |\n",
      "|  | ‡∏™‡∏Å.200 ‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ä‡∏µ‡∏ß‡∏¥‡∏ï‡∏õ‡∏£‡∏∞‡∏à‡∏≥‡∏ß‡∏±‡∏ô | 3 |\n",
      "|  | ‡∏ß‡∏™‡∏´.104 ‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• | 3 |\n",
      "| **‡∏†‡∏≤‡∏Ñ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ó‡∏µ‡πà 2** | ‡∏™‡∏©.105 ‡∏ó‡∏±‡∏Å‡∏©‡∏∞‡∏Å‡∏≤‡∏£‡∏™‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏£‡∏ó‡∏≤‡∏á‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏© | 3 |\n",
      "|  | ‡∏®‡∏®.101 ‡∏Å‡∏≤‡∏£‡∏Ñ‡∏¥‡∏î ‡∏≠‡πà‡∏≤‡∏ô‡πÅ‡∏•‡∏∞‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏°‡∏µ‡∏ß‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏ç‡∏≤‡∏ì | 3 |\n",
      "|  | ‡∏°‡∏ò.155 ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô | 3 |\n",
      "|  | ‡∏™‡∏Å.202 ‡∏ô‡∏ß‡∏±‡∏ï‡∏Å‡∏£‡∏£‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏±‡∏á‡∏Ñ‡∏°‡πÅ‡∏•‡∏∞‡∏ú‡∏π‡πâ‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏±‡∏á‡∏Ñ‡∏° | 3 |\n",
      "|  | ‡∏ß‡∏Ç.201 ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• | 3 |\n",
      "|  | ‡∏ß‡∏Ç.208 ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡∏∞‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ß‡∏¥‡∏ò‡∏µ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• | 3 |\n",
      "\n",
      "> **‡∏£‡∏ß‡∏°‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï** : 18 ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡πÉ‡∏ô‡∏†‡∏≤‡∏Ñ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ó‡∏µ‡πà 1 ‡πÅ‡∏•‡∏∞ 18 ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡πÉ‡∏ô‡∏†‡∏≤‡∏Ñ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ó‡∏µ‡πà 2  \n",
      "\n",
      "**‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏ó‡∏µ‡πà‡∏Ñ‡∏≤‡∏î‡∏´‡∏ß‡∏±‡∏á (YLOs) ‡∏õ‡∏µ‡∏ó‡∏µ‡πà 1**  \n",
      "\n",
      "- ‡∏ô‡∏±‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏î‡πâ‡∏≤‡∏ô‡∏Ñ‡∏ì‡∏¥‡∏ï‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå ‡∏™‡∏≤‡∏Ç‡∏≤‡∏û‡∏µ‡∏ä‡∏Ñ‡∏ì‡∏¥‡∏ï‡πÄ‡∏ä‡∏¥‡∏á‡πÄ‡∏™‡πâ‡∏ô‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô  \n",
      "- ‡∏ô‡∏±‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏î‡πâ‡∏≤‡∏ô‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå ‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏£‡∏∞‡∏ö‡∏ö‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏ã‡∏≠‡∏ü‡∏ï‡πå‡πÅ‡∏ß‡∏£‡πå  \n",
      "- ‡∏ô‡∏±‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏°‡∏µ‡∏ó‡∏±‡∏Å‡∏©‡∏∞‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡∏†‡∏≤‡∏©‡∏≤ Python ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ï‡πà‡∏≤‡∏á ‡πÜ  \n",
      "- ‡∏ô‡∏±‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏°‡∏µ‡∏ó‡∏±‡∏Å‡∏©‡∏∞‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏£‡∏î‡πâ‡∏ß‡∏¢‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©  \n",
      "- ‡∏ô‡∏±‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏°‡∏µ‡∏ó‡∏±‡∏Å‡∏©‡∏∞‡∏Å‡∏≤‡∏£‡∏Ñ‡∏¥‡∏î‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏∞‡∏ö‡∏ö ‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•  \n",
      "\n",
      "**‡∏™‡∏£‡∏∏‡∏õ**  \n",
      "‡πÉ‡∏ô‡∏õ‡∏µ‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏ó‡∏µ‡πà 1 ‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏à‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏ô 12 ‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏†‡∏≤‡∏Ñ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ó‡∏µ‡πà 1 ‡πÅ‡∏•‡∏∞ 2 ‡∏ï‡∏≤‡∏°‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô ‡πÇ‡∏î‡∏¢‡πÄ‡∏ô‡πâ‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏ó‡∏±‡∏Å‡∏©‡∏∞‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏î‡πâ‡∏≤‡∏ô‡∏Ñ‡∏ì‡∏¥‡∏ï‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå ‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏° ‡∏Å‡∏≤‡∏£‡∏™‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏£‡πÅ‡∏•‡∏∞‡∏ó‡∏±‡∏Å‡∏©‡∏∞‡∏î‡∏¥‡∏à‡∏¥‡∏ó‡∏±‡∏• ‡πÅ‡∏•‡∏∞‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏ó‡∏µ‡πà‡∏Ñ‡∏≤‡∏î‡∏´‡∏ß‡∏±‡∏á‡∏£‡∏ß‡∏°‡∏ñ‡∏∂‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡πÉ‡∏ô‡∏Ñ‡∏ì‡∏¥‡∏ï‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå ‡∏û‡∏µ‡∏ä‡∏Ñ‡∏ì‡∏¥‡∏ï‡πÄ‡∏ä‡∏¥‡∏á‡πÄ‡∏™‡πâ‡∏ô ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô ‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå ‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏° Python ‡∏£‡∏ß‡∏°‡∏ñ‡∏∂‡∏á‡∏ó‡∏±‡∏Å‡∏©‡∏∞‡∏Å‡∏≤‡∏£‡∏™‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏£‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏Ñ‡∏¥‡∏î‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Curriculum STRUCTURE-AWARE QA Agent\n",
    "# DSPy + Ollama (FINAL + PROGRESS BAR)\n",
    "# ============================================================\n",
    "\n",
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "import dspy\n",
    "\n",
    "# ============================================================\n",
    "# 1) CONFIG\n",
    "# ============================================================\n",
    "\n",
    "EMBEDDING_API = \"http://localhost:11434/api/embeddings\"\n",
    "EMBED_MODEL = \"bge-m3:567m\"\n",
    "\n",
    "LLM_URL = \"https://api.ollama.services.storemesh.com/\"\n",
    "MODEL_NAME = \"gpt-oss:20b\"\n",
    "\n",
    "TOP_K = 6\n",
    "\n",
    "# ============================================================\n",
    "# 2) EMBEDDING (WITH PROGRESS BAR)\n",
    "# ============================================================\n",
    "\n",
    "def embed_texts(texts: list[str]) -> np.ndarray:\n",
    "    vectors = []\n",
    "\n",
    "    for t in tqdm(\n",
    "        texts,\n",
    "        desc=\"üß† Creating embeddings\",\n",
    "        unit=\"chunk\"\n",
    "    ):\n",
    "        r = requests.post(\n",
    "            EMBEDDING_API,\n",
    "            json={\n",
    "                \"model\": EMBED_MODEL,\n",
    "                \"prompt\": t\n",
    "            },\n",
    "            timeout=120\n",
    "        )\n",
    "        r.raise_for_status()\n",
    "        vectors.append(r.json()[\"embedding\"])\n",
    "\n",
    "    return np.array(vectors)\n",
    "\n",
    "# ============================================================\n",
    "# 3) BUILD CORPUS FROM DF (WITH PROGRESS BAR)\n",
    "# ============================================================\n",
    "\n",
    "def build_corpus(df: pd.DataFrame):\n",
    "    texts, meta = [], []\n",
    "\n",
    "    for _, r in tqdm(\n",
    "        df.iterrows(),\n",
    "        total=len(df),\n",
    "        desc=\"üìò Building retrieval corpus\",\n",
    "        unit=\"block\"\n",
    "    ):\n",
    "        blob = f\"\"\"\n",
    "[SECTION] {r['section']}\n",
    "[TYPE] {r['block_type']}\n",
    "[TEXT]\n",
    "{r['text']}\n",
    "\"\"\".strip()\n",
    "\n",
    "        texts.append(blob)\n",
    "        meta.append({\n",
    "            \"section\": r[\"section\"],\n",
    "            \"page\": r[\"page\"],\n",
    "            \"block_type\": r[\"block_type\"]\n",
    "        })\n",
    "\n",
    "    return texts, meta\n",
    "\n",
    "# ============================================================\n",
    "# 4) RETRIEVER\n",
    "# ============================================================\n",
    "\n",
    "class Retriever:\n",
    "    def __init__(self, texts, meta, embeddings):\n",
    "        self.texts = texts\n",
    "        self.meta = meta\n",
    "        self.embeddings = embeddings\n",
    "\n",
    "    def search(self, query: str, k: int = TOP_K):\n",
    "        q_emb = embed_texts([query])\n",
    "        sims = cosine_similarity(q_emb, self.embeddings)[0]\n",
    "        idx = sims.argsort()[-k:][::-1]\n",
    "\n",
    "        return [\n",
    "            {\n",
    "                \"text\": self.texts[i],\n",
    "                \"meta\": self.meta[i],\n",
    "                \"score\": float(sims[i])\n",
    "            }\n",
    "            for i in idx\n",
    "        ]\n",
    "\n",
    "# ============================================================\n",
    "# 5) PASS 1 ‚Äî STRUCTURE EXTRACTION AGENT\n",
    "# ============================================================\n",
    "\n",
    "class CurriculumStructureSignature(dspy.Signature):\n",
    "    \"\"\"\n",
    "    Extract curriculum structure as JSON.\n",
    "    JSON ONLY. No explanation.\n",
    "    \"\"\"\n",
    "    context = dspy.InputField()\n",
    "    structure = dspy.OutputField()\n",
    "\n",
    "\n",
    "class CurriculumStructureAgent(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.predict = dspy.Predict(CurriculumStructureSignature)\n",
    "\n",
    "    def forward(self, context: str):\n",
    "        return self.predict(context=context)\n",
    "\n",
    "# ============================================================\n",
    "# 6) BUILD GLOBAL CONTEXT (WITH PROGRESS BAR)\n",
    "# ============================================================\n",
    "\n",
    "def build_global_context(df: pd.DataFrame, max_chars=12000):\n",
    "    chunks = []\n",
    "\n",
    "    for _, r in tqdm(\n",
    "        df.iterrows(),\n",
    "        total=len(df),\n",
    "        desc=\"üìê Building global curriculum context\",\n",
    "        unit=\"block\"\n",
    "    ):\n",
    "        chunks.append(\n",
    "            f\"[{r['block_type']} | {r['section']}]\\n{r['text']}\"\n",
    "        )\n",
    "\n",
    "    return \"\\n\\n\".join(chunks)[:max_chars]\n",
    "\n",
    "# ============================================================\n",
    "# 7) PASS 2 ‚Äî QA SIGNATURE\n",
    "# ============================================================\n",
    "\n",
    "class CurriculumQASignature(dspy.Signature):\n",
    "    \"\"\"\n",
    "   ‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠ ‚Äú‡∏ö‡∏£‡∏£‡∏ì‡∏≤‡∏£‡∏±‡∏Å‡∏©‡πå‡∏î‡∏¥‡∏à‡∏¥‡∏ó‡∏±‡∏• (Digital Librarian Agent)‚Äù\n",
    "‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠:\n",
    "- ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤\n",
    "- ‡∏Ñ‡∏±‡∏î‡∏Å‡∏£‡∏≠‡∏á\n",
    "- ‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á\n",
    "- ‡πÅ‡∏•‡∏∞‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n",
    "‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏°‡∏≤‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏∞‡∏ö‡∏ö‡πÅ‡∏•‡∏∞‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÑ‡∏î‡πâ\n",
    "\n",
    "‡∏Å‡∏é‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö:\n",
    "1. ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å CONTEXT ‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏°‡∏≤‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô\n",
    "2. ‡∏´‡πâ‡∏≤‡∏°‡πÉ‡∏ä‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏†‡∏≤‡∏¢‡∏ô‡∏≠‡∏Å ‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏≤‡∏î‡πÄ‡∏î‡∏≤‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°\n",
    "3. ‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ ‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡∏ß‡πà‡∏≤:\n",
    "   \"‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£\"\n",
    "4. ‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢\n",
    "5. ‡∏à‡∏±‡∏î‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏∞‡∏ö‡∏ö ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô ‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏•‡∏≤‡∏á‡πÉ‡∏ô‡πÄ‡∏ä‡∏¥‡∏á‡∏ß‡∏¥‡∏ä‡∏≤‡∏Å‡∏≤‡∏£\n",
    "6. ‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏©‡∏≤‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏†‡∏≤‡∏û ‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥ ‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡πÄ‡∏ä‡∏¥‡∏á‡∏™‡∏≤‡∏£‡∏™‡∏ô‡πÄ‡∏ó‡∏®‡πÅ‡∏ö‡∏ö‡∏ö‡∏£‡∏£‡∏ì‡∏≤‡∏£‡∏±‡∏Å‡∏©‡πå\n",
    "\n",
    "‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥:\n",
    "- ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠ / ‡∏õ‡∏£‡∏∞‡πÄ‡∏î‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å\n",
    "- ‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£\n",
    "- (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ) ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ï‡∏≤‡∏°‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£\n",
    "    \"\"\"\n",
    "    question = dspy.InputField()\n",
    "    context = dspy.InputField()\n",
    "    answer = dspy.OutputField()\n",
    "\n",
    "# ============================================================\n",
    "# 8) STRUCTURE-AWARE QA AGENT (ReAct)\n",
    "# ============================================================\n",
    "\n",
    "class CurriculumQAAgent(dspy.Module):\n",
    "    def __init__(self, retriever, curriculum_structure: str):\n",
    "        super().__init__()\n",
    "        self.retriever = retriever\n",
    "        self.curriculum_structure = curriculum_structure\n",
    "\n",
    "        self.react = dspy.ReAct(\n",
    "            CurriculumQASignature,\n",
    "            tools=[]\n",
    "        )\n",
    "\n",
    "    def forward(self, question: str):\n",
    "        docs = self.retriever.search(question)\n",
    "\n",
    "        retrieved = \"\\n\\n\".join(\n",
    "            f\"[{d['meta']['section']} | page {d['meta']['page']}]\\n{d['text']}\"\n",
    "            for d in docs\n",
    "        )\n",
    "\n",
    "        context = f\"\"\"\n",
    "### CURRICULUM STRUCTURE (GROUND TRUTH)\n",
    "{self.curriculum_structure}\n",
    "\n",
    "### RELEVANT DOCUMENTS\n",
    "{retrieved}\n",
    "\"\"\".strip()\n",
    "\n",
    "        return self.react(\n",
    "            question=question,\n",
    "            context=context\n",
    "        )\n",
    "\n",
    "# ============================================================\n",
    "# 9) MAIN PIPELINE (WITH PROGRESS BAR)\n",
    "# ============================================================\n",
    "\n",
    "def main(df: pd.DataFrame):\n",
    "    # -----------------------------\n",
    "    # Init LLM\n",
    "    # -----------------------------\n",
    "    print(\"ü§ñ Initializing LLM...\")\n",
    "    lm = dspy.LM(\n",
    "        f\"ollama/{MODEL_NAME}\",\n",
    "        api_base=LLM_URL,\n",
    "        cache=False\n",
    "    )\n",
    "    dspy.configure(lm=lm)\n",
    "\n",
    "    # -----------------------------\n",
    "    # PASS 1: Extract Structure\n",
    "    # -----------------------------\n",
    "    print(\"\\nüß† Extracting curriculum structure...\")\n",
    "    global_context = build_global_context(df)\n",
    "\n",
    "    structure_agent = CurriculumStructureAgent()\n",
    "    with tqdm(total=1, desc=\"üß† Curriculum structure reasoning\") as pbar:\n",
    "        structure_result = structure_agent(context=global_context)\n",
    "        pbar.update(1)\n",
    "\n",
    "    curriculum_structure = structure_result.structure\n",
    "\n",
    "    print(\"\\nüìê CURRICULUM STRUCTURE (JSON):\")\n",
    "    print(curriculum_structure)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Build Retriever\n",
    "    # -----------------------------\n",
    "    print(\"\\nüìò Building retrieval index...\")\n",
    "    corpus_texts, corpus_meta = build_corpus(df)\n",
    "    embeddings = embed_texts(corpus_texts)\n",
    "\n",
    "    retriever = Retriever(\n",
    "        corpus_texts,\n",
    "        corpus_meta,\n",
    "        embeddings\n",
    "    )\n",
    "\n",
    "    # -----------------------------\n",
    "    # PASS 2: QA Agent\n",
    "    # -----------------------------\n",
    "    agent = CurriculumQAAgent(\n",
    "        retriever,\n",
    "        curriculum_structure\n",
    "    )\n",
    "\n",
    "    print(\"\\n‚úÖ Structure-aware Curriculum QA Agent ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô\")\n",
    "    print(\"‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£ (exit ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏≠‡∏≠‡∏Å)\\n\")\n",
    "\n",
    "    while True:\n",
    "        q = input(\"‚ùì ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: \").strip()\n",
    "        if q.lower() in [\"exit\", \"quit\"]:\n",
    "            break\n",
    "\n",
    "        result = agent(question=q)\n",
    "        print(\"\\nüìå ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö:\")\n",
    "        print(result.answer)\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "# ============================================================\n",
    "# 10) ENTRY\n",
    "# ============================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    df ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ column:\n",
    "    - text\n",
    "    - section\n",
    "    - page\n",
    "    - block_type\n",
    "    \"\"\"\n",
    "\n",
    "    df = df_str   # ‚Üê ‡πÉ‡∏™‡πà DataFrame ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ\n",
    "    main(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
